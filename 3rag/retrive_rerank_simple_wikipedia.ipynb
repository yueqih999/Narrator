{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueqih/github-classroom/StoryLens/StoryLens/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No GPU found. Please add GPU to your notebook\n",
      "Passages: 169597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 10.50it/s]\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "\n",
    "#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "# about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        #Add all paragraphs\n",
    "        #passages.extend(data['paragraphs'])\n",
    "\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))\n",
    "passages = passages[:100]\n",
    "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 100800.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# We lower case our text and remove stop-words from indexing\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Semantic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: What is the capital of the United States?\n",
      "Top-3 lexical search (BM25) hits\n",
      "\t7.865\tWilmington is a city in New Hanover County, North Carolina, United States.\n",
      "\t5.637\tPlantation Key is an island in Monroe County, Florida, United States. It is in the upper Florida Keys on U.S. Route 1 (or the \"Overseas Highway\"), between Key Largo and Windley Key.\n",
      "\t5.495\tThe Gifford Pinchot National Forest is a United States National Forest in Washington, USA. It has of land. The forest includes the Mount St. Helens National Volcanic Monument, which takes up of the property.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of the United States?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m##### Semantic Search #####\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Encode the query using the bi-encoder and find potentially relevant passages\u001b[39;00m\n\u001b[1;32m     18\u001b[0m question_embedding \u001b[38;5;241m=\u001b[39m bi_encoder\u001b[38;5;241m.\u001b[39mencode(query, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m question_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mquestion_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m hits \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39msemantic_search(question_embedding, corpus_embeddings, top_k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[1;32m     21\u001b[0m hits \u001b[38;5;241m=\u001b[39m hits[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Get the hits for the first query\u001b[39;00m\n",
      "File \u001b[0;32m~/github-classroom/StoryLens/StoryLens/venv/lib/python3.13/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "search(query = \"What is the capital of the United States?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
